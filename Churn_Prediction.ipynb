{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyAugroZQolk"
      },
      "source": [
        "#**Churn Prediction using Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-hm8Oy3Z4Qr"
      },
      "source": [
        "**Customer Churn Prediction**\n",
        "\n",
        "This project builds and compares machine learning models (XGBoost, Random Forest, Logistic Regression)  \n",
        "to predict customer churn using structured data.  \n",
        "\n",
        "**Goal:** Identify which customers are likely to leave, enabling proactive retention strategies.\n",
        "\n",
        "**Tech Stack:** Python, scikit-learn, XGBoost, pandas, matplotlib, seaborn.  \n",
        "**Key Techniques:** Imbalance handling, RandomizedSearchCV optimization, ROC-AUC evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv3U3wyoYfZa"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fEi-9kLlQOIm"
      },
      "outputs": [],
      "source": [
        "#Requirements\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dGaIQkIOwZn",
        "outputId": "12b10006-a441-43c6-8678-47175567fcaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Samples: 6812\n",
            "Churn (1) count: 1352\n",
            "No Churn (0) count: 5460\n",
            "--------------------------------------------------\n",
            "Training set size: 5449 | Test set size: 1363\n",
            "--------------------------------------------------\n",
            "Calculated scale_pos_weight: 4.04\n",
            "This tells model to pay 4.04 times more attention to churn cases.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Dataset setup\n",
        "df = pd.read_csv('Customer_Churn_Processed.csv')\n",
        "X = df.drop('ChurnStatus', axis=1)\n",
        "y = df['ChurnStatus']\n",
        "N_SAMPLES = len(df)\n",
        "\n",
        "print(f\"Total Samples: {N_SAMPLES}\")\n",
        "print(f\"Churn (1) count: {sum(y == 1)}\")\n",
        "print(f\"No Churn (0) count: {sum(y == 0)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 2. DATA SPLIT ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Training set size: {len(X_train)} | Test set size: {len(X_test)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 3. IMBALANCE HANDLING: CALCULATE SCALE_POS_WEIGHT ---\n",
        "# XGBoost handles imbalance by weighting the minority class during training.\n",
        "# scale_pos_weight = (Count of Negative Samples) / (Count of Positive Samples)\n",
        "neg_count = sum(y_train == 0)\n",
        "pos_count = sum(y_train == 1)\n",
        "scale_pos_weight_ratio = neg_count / pos_count\n",
        "\n",
        "print(f\"Calculated scale_pos_weight: {scale_pos_weight_ratio:.2f}\")\n",
        "print(\"This tells model to pay {:.2f} times more attention to churn cases.\".format(scale_pos_weight_ratio))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "results = {'XGBoost':[], 'Random Forest':[], 'Logistic Regression':[]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX7xBd5HQ2ib"
      },
      "source": [
        "##XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSgCos7iPReU",
        "outputId": "0e530d14-adf9-4f46-f6a6-86aac08bc571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Randomized Search for XGBoost (Optimizing for ROC AUC)...\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:35:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomized Search Complete.\n",
            "--------------------------------------------------\n",
            "Best Hyperparameters Found:\n",
            "{'colsample_bytree': np.float64(0.6943939678995823), 'gamma': np.float64(0.12803416138066198), 'learning_rate': np.float64(0.022130076861529402), 'max_depth': 9, 'n_estimators': 114, 'reg_lambda': np.float64(0.7217816416236627), 'subsample': np.float64(0.7757346007463081)}\n",
            "\n",
            "Best CV ROC AUC Score: 0.9874894549487367\n",
            "--------------------------------------------------\n",
            "Evaluating Final XGBoost Model on Test Set...\n",
            "Test Set ROC AUC: 0.9964\n",
            "\n",
            "Classification Report on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1092\n",
            "           1       0.97      0.96      0.97       271\n",
            "\n",
            "    accuracy                           0.99      1363\n",
            "   macro avg       0.98      0.98      0.98      1363\n",
            "weighted avg       0.99      0.99      0.99      1363\n",
            "\n",
            "Confusion Matrix:\n",
            "              Predicted No Churn | Predicted Churn\n",
            "Actual No Churn:      1085          |      7\n",
            "Actual Churn:         10           |      261\n"
          ]
        }
      ],
      "source": [
        "#Initialize XGBoost Classifier with imbalance handling and reproducibility settings\n",
        "xgb = XGBClassifier(\n",
        "    random_state=42,\n",
        "    # Use the calculated ratio to manually balance the classes\n",
        "    scale_pos_weight=scale_pos_weight_ratio,\n",
        "    # Standard settings for classification\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# --- 5. HYPERPARAMETER SEARCH SPACE (DISTRIBUTIONS FOR RANDOMIZED SEARCH) ---\n",
        "# Define broad distributions for the most impactful XGBoost hyperparameters\n",
        "param_distributions = {\n",
        "    # Number of boosting rounds/trees (integer distribution)\n",
        "    'n_estimators': randint(100, 600),\n",
        "    # Maximum depth of a tree (integer distribution)\n",
        "    'max_depth': randint(3, 10),\n",
        "    # Step size shrinkage used in updates to prevent overfitting (log-uniform distribution)\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    # L2 regularization term on weights (useful for weak signals)\n",
        "    'reg_lambda': uniform(0.5, 2),\n",
        "    # Minimum loss reduction required to make a further partition (controls pruning)\n",
        "    'gamma': uniform(0, 0.5),\n",
        "    # Subsample ratio of the training instance (fraction)\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    # Subsample ratio of columns when constructing each tree\n",
        "    'colsample_bytree': uniform(0.6, 0.4)\n",
        "}\n",
        "\n",
        "# --- 6. RANDOMIZED SEARCH CV SETUP AND EXECUTION ---\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,             # Number of different combinations to try\n",
        "    cv=5,                  # 5-fold cross-validation\n",
        "    scoring='roc_auc',     # Optimize for the robust ROC AUC metric\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1              # Use all available CPU cores\n",
        ")\n",
        "\n",
        "print(\"Starting Randomized Search for XGBoost (Optimizing for ROC AUC)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Randomized Search Complete.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 7. FINAL MODEL EVALUATION ---\n",
        "best_xgb = random_search.best_estimator_\n",
        "\n",
        "# A. Print Best Parameters and ROC AUC Score\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(\"\\nBest CV ROC AUC Score:\", random_search.best_score_)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# B. Evaluate on the Test Set\n",
        "print(\"Evaluating Final XGBoost Model on Test Set...\")\n",
        "\n",
        "# Get probability predictions (needed for ROC AUC)\n",
        "y_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
        "test_roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"Test Set ROC AUC: {test_roc_auc:.4f}\")\n",
        "results['XGBoost'] = float(test_roc_auc)\n",
        "\n",
        "# Get hard predictions (needed for Classification Report)\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# C. Confusion Matrix (Visualizing true positives/negatives)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"              Predicted No Churn | Predicted Churn\")\n",
        "print(f\"Actual No Churn:      {cm[0, 0]}          |      {cm[0, 1]}\")\n",
        "print(f\"Actual Churn:         {cm[1, 0]}           |      {cm[1, 1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D782JEBRYSF"
      },
      "source": [
        "##Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-lqWKZvRanj",
        "outputId": "0259e71c-5769-4bb3-ac0b-8da9e7fd6c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Randomized Search for Random Forest (Optimizing for ROC AUC)...\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Randomized Search Complete.\n",
            "--------------------------------------------------\n",
            "Best Hyperparameters Found:\n",
            "{'bootstrap': False, 'max_depth': 14, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 11, 'n_estimators': 575}\n",
            "\n",
            "Best CV ROC AUC Score: 0.9943363790651544\n",
            "--------------------------------------------------\n",
            "Evaluating Final Random Forest Model on Test Set...\n",
            "Test Set ROC AUC: 0.9961\n",
            "\n",
            "Classification Report on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1092\n",
            "           1       1.00      0.97      0.98       271\n",
            "\n",
            "    accuracy                           0.99      1363\n",
            "   macro avg       0.99      0.98      0.99      1363\n",
            "weighted avg       0.99      0.99      0.99      1363\n",
            "\n",
            "Confusion Matrix:\n",
            "              Predicted No Churn | Predicted Churn\n",
            "Actual No Churn:      1091          |      1\n",
            "Actual Churn:         9           |      262\n"
          ]
        }
      ],
      "source": [
        "#Initialize Random Forest Classifier with class weighting for imbalance\n",
        "rf = RandomForestClassifier(\n",
        "    random_state=42,\n",
        "    class_weight='balanced',  # Automatically handle class imbalance\n",
        "    n_jobs=-1                 # Use all CPU cores\n",
        ")\n",
        "\n",
        "# --- 5. HYPERPARAMETER SEARCH SPACE (DISTRIBUTIONS FOR RANDOMIZED SEARCH) ---\n",
        "# Define broad distributions for key Random Forest hyperparameters\n",
        "param_distributions = {\n",
        "    # Number of trees in the forest\n",
        "    'n_estimators': randint(100, 600),\n",
        "    # Maximum depth of each tree\n",
        "    'max_depth': randint(3, 20),\n",
        "    # Minimum number of samples required to split an internal node\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    # Minimum number of samples required to be at a leaf node\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    # Number of features to consider when looking for the best split\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    # Whether bootstrap samples are used when building trees\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# --- 6. RANDOMIZED SEARCH CV SETUP AND EXECUTION ---\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,             # Number of different combinations to try\n",
        "    cv=5,                  # 5-fold cross-validation\n",
        "    scoring='roc_auc',     # Optimize for the robust ROC AUC metric\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1              # Use all available CPU cores\n",
        ")\n",
        "\n",
        "print(\"Starting Randomized Search for Random Forest (Optimizing for ROC AUC)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Randomized Search Complete.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 7. FINAL MODEL EVALUATION ---\n",
        "best_rf = random_search.best_estimator_\n",
        "\n",
        "# A. Print Best Parameters and ROC AUC Score\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(\"\\nBest CV ROC AUC Score:\", random_search.best_score_)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# B. Evaluate on the Test Set\n",
        "print(\"Evaluating Final Random Forest Model on Test Set...\")\n",
        "\n",
        "# Get probability predictions (needed for ROC AUC)\n",
        "y_proba = best_rf.predict_proba(X_test)[:, 1]\n",
        "test_roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"Test Set ROC AUC: {test_roc_auc:.4f}\")\n",
        "results['Random Forest'] = float(test_roc_auc)\n",
        "\n",
        "# Get hard predictions (needed for Classification Report)\n",
        "y_pred = best_rf.predict(X_test)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# C. Confusion Matrix (Visualizing true positives/negatives)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"              Predicted No Churn | Predicted Churn\")\n",
        "print(f\"Actual No Churn:      {cm[0, 0]}          |      {cm[0, 1]}\")\n",
        "print(f\"Actual Churn:         {cm[1, 0]}           |      {cm[1, 1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew9xueWNTiUW"
      },
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjPFhLrLTlnK",
        "outputId": "e01c25d8-7b2a-4c2b-ba69-ba26a14a0187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Randomized Search for Logistic Regression (Optimizing for ROC AUC)...\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Randomized Search Complete.\n",
            "--------------------------------------------------\n",
            "Best Hyperparameters Found:\n",
            "{'C': np.float64(1.988156815341724), 'l1_ratio': np.float64(0.005522117123602399), 'penalty': 'elasticnet'}\n",
            "\n",
            "Best CV ROC AUC Score: 0.6059807156117595\n",
            "--------------------------------------------------\n",
            "Evaluating Final Logistic Regression Model on Test Set...\n",
            "Test Set ROC AUC: 0.5899\n",
            "\n",
            "Classification Report on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.60      0.70      1092\n",
            "           1       0.24      0.51      0.32       271\n",
            "\n",
            "    accuracy                           0.58      1363\n",
            "   macro avg       0.53      0.55      0.51      1363\n",
            "weighted avg       0.71      0.58      0.62      1363\n",
            "\n",
            "Confusion Matrix:\n",
            "              Predicted No Churn | Predicted Churn\n",
            "Actual No Churn:      654          |      438\n",
            "Actual Churn:         134           |      137\n"
          ]
        }
      ],
      "source": [
        "#Initialize Logistic Regression with class weighting for imbalance\n",
        "log_reg = LogisticRegression(\n",
        "    random_state=42,\n",
        "    class_weight='balanced',   # Automatically adjust weights inversely to class frequencies\n",
        "    solver='saga',             # Supports L1/L2 regularization and elastic net\n",
        "    max_iter=5000,             # Ensure convergence for larger datasets\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# --- 5. HYPERPARAMETER SEARCH SPACE (DISTRIBUTIONS FOR RANDOMIZED SEARCH) ---\n",
        "# Define broad distributions for Logistic Regression hyperparameters\n",
        "param_distributions = {\n",
        "    # Regularization strength (inverse of regularization coefficient)\n",
        "    # Smaller C = stronger regularization\n",
        "    'C': uniform(0.001, 10),\n",
        "    # Type of penalty (L1 = Lasso, L2 = Ridge)\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    # Elastic net mixing parameter (only used if penalty='elasticnet')\n",
        "    'l1_ratio': uniform(0, 1)\n",
        "}\n",
        "\n",
        "# --- 6. RANDOMIZED SEARCH CV SETUP AND EXECUTION ---\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,             # Number of different combinations to try\n",
        "    cv=5,                  # 5-fold cross-validation\n",
        "    scoring='roc_auc',     # Optimize for ROC AUC\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Starting Randomized Search for Logistic Regression (Optimizing for ROC AUC)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Randomized Search Complete.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 7. FINAL MODEL EVALUATION ---\n",
        "best_log_reg = random_search.best_estimator_\n",
        "\n",
        "# A. Print Best Parameters and ROC AUC Score\n",
        "print(\"Best Hyperparameters Found:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(\"\\nBest CV ROC AUC Score:\", random_search.best_score_)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# B. Evaluate on the Test Set\n",
        "print(\"Evaluating Final Logistic Regression Model on Test Set...\")\n",
        "\n",
        "# Get probability predictions (needed for ROC AUC)\n",
        "y_proba = best_log_reg.predict_proba(X_test)[:, 1]\n",
        "test_roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"Test Set ROC AUC: {test_roc_auc:.4f}\")\n",
        "results['Logistic Regression'] = float(test_roc_auc)\n",
        "\n",
        "# Get hard predictions (needed for Classification Report)\n",
        "y_pred = best_log_reg.predict(X_test)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# C. Confusion Matrix (Visualizing true positives/negatives)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"              Predicted No Churn | Predicted Churn\")\n",
        "print(f\"Actual No Churn:      {cm[0, 0]}          |      {cm[0, 1]}\")\n",
        "print(f\"Actual Churn:         {cm[1, 0]}           |      {cm[1, 1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79nOS3JWZHHv"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xuc1nxbZKRh",
        "outputId": "0c6c218a-944c-4a5e-83f3-3bbd55e1ae89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost ROC AUC: 0.9964\n",
            "Random Forest ROC AUC: 0.9961\n",
            "Logistic Regression ROC AUC: 0.5899\n"
          ]
        }
      ],
      "source": [
        "for model, roc_auc in results.items():\n",
        "    print(f\"{model} ROC AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m922p3v5ZN4i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}